import re


def tokenize():
    file_name = "smallExample.txt"

    # Opens the specified file
    file = open(file_name)

    # Stores all of the words read in from the input file
    words = []

    for line in file:
        for token in re.split('[-.,]', line):
            words.append(token)




    for tks in words:
        print('\n' + tks)

if __name__ == '__main__':
    tokenize()