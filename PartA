import re


def tokenize():
    file_name = "smallExample.txt"

    # Opens the specified file
    file = open(file_name)

    # Stores all of the words read in from the input file
    words = []

    # We split on anything that is not a comma, letter (case sensitive), or apostrophe
    for line in file:
        for token in re.split('[^a-zA-Z0-9\']', line):
            words.append(token)

    return words

def computeWordFrequencies():

    hello = ""


if __name__ == '__main__':
    tokenize()